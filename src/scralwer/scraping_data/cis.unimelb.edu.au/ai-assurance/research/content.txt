URL: https://cis.unimelb.edu.au/ai-assurance/research








Research, AI Assurance Lab, Computing and Information Systems, The University of Melbourne













































        Skip to main content
      





School of Computing and Information Systems



AI Assurance



Research







Research






We work with wide range of industries, ranging from cyber security and defense to finance and smart cities.The Lab provides a set of practical testbeds for evaluating AI algorithms such as machine learning algorithms. Such evaluation includes assessment of the appropriateness of training datasets; evaluation of the robustness of AI algorithms to varying or unexpected inputs; assessment of the privacy levels of AI models; resilience of AI systems to adversarial attacks; competency of AI algorithms to handle new scenarios.FacilitiesWe provide a set of practical testbeds for evaluating Machine Learning (ML) models:Explanation testbed: generating human-interpretable explanations of ML models.Privacy testbed: generating examples to test resistance of ML models to attacks that extract private information encoded by the model from the training data.Adversarial testbeds: generating adversarial examples to test resistance of ML models to attacks or distribution shifts.Variation testbeds: generating examples to test the robustness of ML models to new types of inputs.The Lab’s capability is driven by approximately 40 academic staff at the University of Melbourne who are working directly in AI, machine learning or their interface to areas of computing such as software engineering, business processes, human computer interaction and cyber security.Explainable AIAs AI has becomes more ubiquitous, complex and consequential, the need for people to understand how decisions are made and to judge their correctness, fairness, and transparency, has become increasingly crucial due to concerns of ethics and trust.The field of Explainable AI (XAI), aims to address this problem by designing intelligent agents whose decisions and actions building up to these decisions can be easily understood by humans.Truly explainable AI requires integration of the technical and human challenges. To make progress, we need a sophisticated understanding of what constitutes “explanation” in an AI context; one that is not merely limited to an articulation of the software processes, but explicitly considers that it is ultimately people that will consume these explanations. The research into explainable AI in our research laboratory at the University of Melbourne is aiming to build explainable AI from these very principles.We aim to take a human-centered approach to XAI, explicitly studying what questions people care about for explanation, what makes a good explanation to a person, how explanations and causes can be extracted from complex and often opaque decision-making models, and how they can be communicated to people.This research effort is a collaborative project involving computer science, cognitive science, social psychology, and human-computer interaction, treating explainable AI as an interaction between person and machine.Privacy-enhancing technologyWe explore technical solutions to enable data analysis with strong security and privacy guarantees – critical when using running machine learning or a database over sensitive data. Our research areas include differential privacy for strong privacy guarantees when releasing an output of data analysis (eg, through statistics about the data or a machine learning model trained on it), confidential data processing (eg, by relying on cryptographic techniques and Trusted Execution Environments), privacy risk analysis, and questions of privacy, security and verification that arise when computing over multi-party data.We engage across industry and government, with past research contracts with the Australian Bureau of Statistics, Facebook, the Office of the Victorian Information Commissioner, and Transport for New South Wales as recent examples.Adversarial Machine LearningAI and ML techniques have made great advances in automating the detection of malicious behaviour in data-intensive applications such as cyber security, defence, fraud detection and border security. For example, ML techniques such as anomaly detection can detect abnormal behaviour that may arise due to malicious activities, such fraudulent financial transactions. However, a major challenge to using ML in this context arises when the adversaries we are trying to detect know that they are being monitored, and thus modify their behaviour to “poison” the training of the ML models. This challenge has motivated research into Adversarial Machine Learning, which aims to develop robust ML techniques that a resilient against the attempts of intelligent adversaries to manipulate the ML-based systems. In particular, there is a need for assurance platforms to test whether a given ML system is vulnerable to these types of adversarial attacks.The University of Melbourne (with support from Defence’s Next Generation Technology Fund) has developed a range of techniques for assurance of ML systems in adversarial environments. We have developed a variety of new technologies to test the susceptibility of a given ML system to adversarial attacks, such as contaminating training data, crafting adversarial test cases that can fool a system, or generating privacy attacks that can extract private information from an ML model. This has led to practical tool sets that can be used for assurance testing of ML models against adversarial attacks in practical applications, such as cyber security, defence and fraud detection.The AI Assurance Lab draws upon the extensive practical and theoretical experience of the Faculty of Engineering and Information Technology. It builds on our research collaboration with organisations such as the Defence Science and Technology Group on using Adversarial Machine Learning for Cyber Security. It also draws upon the expertise of the University of Melbourne Academic Centre for Cyber Security Excellence, which is one of two such centres funded by the Commonwealth Government to advance research and education in Cyber Security, especially in the use of AI and ML for security analytics in adversarial environments.Digital EthicsAdvances in digital technologies have increasing ethical challenges which needs to be considered by stakeholders, including the scientists and engineers involved in their development.Centre for Artificial Intelligence and Digital EthicsCombining expertise from Melbourne Law School, the Faculty of Engineering and Information Technology, the Faculty of Arts, and the Faculty of Science, the Centre for Artificial Intelligence and Digital Ethics (CAIDE) bring interdisciplinary insights to AI and digital ethics with a uniquely Australian focus.The activities of CAIDE revolve around four themes: fairness, accountability, transparency and privacy.CAIDE is leading research, teaching and engagement on these themes, addressing each in the context of challenges and opportunities presented in Australia. The AI Assurance Lab activities are being shaped by ethical principles developed by collaborators at CAIDE. This includes consideration of important issues such as fairness and anti-discrimination, auditing and transparency, accountability and governance, and consent and data privacy.




Email Contact us
Faculty of Engineering and Information Technology
Engineering & IT Students
Staff only: FEIT Intranet







School of Computing and Information Systems
About us
About the School
Life in Melbourne
Alumni profiles
Casual tutor, demonstrator, marker and project team supervisor opportunities
CSIRAC: Our first computer
CSIRAC’s vital statistics
CSIRAC chronology
CSIRAC design
How did CSIRAC work?: Storage
How did CSIRAC work?: Console
CSIRAC uses
CSIRAC: Designers
The music of CSIRAC
CSIRAC emulator
Jurij Semkiw
CSIRAC and computer history links
The Last of the First, CSIRAC: Australia’s First Computer
CSIRAC photo gallery
The history of computing at the University of Melbourne
History of computing in the department
Starting the Department of Information Systems
Early internet
Memories of the department
Programming
Student life in the department
Women in computing
CIS Doctoral Colloquium
2023 CIS Doctoral Colloquium
Submission Guidelines
Colloquium Photos
2022 CIS Doctoral Colloquium
Submission Guidelines
Program_open
Participation and Awards
Volunteers and Judges
CISDC 2022 Photos
2019 CIS Doctoral Colloquium
2018 CIS Doctoral Colloquium
2017 CIS Doctoral Colloquium
2016 CIS Doctoral Colloquium
2015 CIS Doctoral Colloquium
2014 CIS Doctoral Colloquium
2013 CIS Doctoral Colloquium
Keynote speaker
Colloquium sponsors
Committee
Call for papers
Application_closed
Submission_info
Program_pending
Program_open
Alumni
Registration_pending
Industry
Research collaboration
Cyber attack maps to underpin better strategic responses
Surgeons gain implant expertise with virtual training
Informatics analyses value in digital health technologies
Data contrasting highlights changing use of city
Online community designed to support mental health for young people
Satisfaction score to improve quality of internet search results
Supply chain scheduling keeps automated mining operations on task
Combined data adds power to decision-making
New algorithms help interpret vision loss from digital images
Host a student intern
Mentor our students
Student industry projects
Become a guest speaker
News
2022 news and events
CIS-EEE Seed Funding
How data can prevent overdiagnosis

CIS - IE 2022 Research Collaboration Seed Funding
2021 news and events
2020 news and events
2019 news and events
2023 news and events
CIS-ME 2023 Seed Funding round
CIS-IE 2023 Seed Funding round
People
Research
Artificial intelligence
Graduate Reseachers
Computer science
Graduate Researchers
Digital Health
Research projects
Seminars
Schools
Study with us
Undergraduate programs
Graduate coursework programs
Graduate research programs
Industry based learning
Programming proficiency test
First Year Centre
About
Study with CIS
Undergraduate programs
Get help
People
Become a tutor
Contact
Graduate programs
Graduate coursework programs
Graduate research programs
AI and Autonomy Lab
News
People
Publications
Research
Mining and optimisation
Automated planning languages
Foundations of human-agent collaboration
People-oriented software engineering
Explainable artificial intelligence
Multimodal human-agent collaboration
Artificial Intelligence Assurance Lab
About us
Research
Publications
People
Industry engagement
Academic Centre of Cyber Security Excellence (ACCSE)
Human-Computer Interaction
Facilities
User Experience Lab
Interactive Technologies Lab
Engineering Workshop

Telstra Creator Space

News and Events
People
Staff
Graduate researchers
Alumni
Projects
Adaptive learning technologies
Ageing and avatars
AI-enabled assistance for strategic planning in games
Wearable technology for arm monitoring in health
Augmented fitness
Augmented learning environment for physiotherapy education
Biometric Mirror
Changing views
Citizen Heritage
Cognition-aware systems
Cognitive interaction
Completed projects
Connecting learners for collaboration across diverse communities
Cross-community information systems
Crowdsourcing
Death and the Internet
Deceptive AI
Designing for scale
Designing technologies for indigenous knowledge
Digital commemoration
Digital domesticity
Conceptualising and measuring digital emotion regulation
Emerging technologies for enrichment in old age
Encounters
Ethics and digital games
Evaluation of natural user interfaces in query auto-completion
Examining the ‘digital’ in hybrid digital boardgames
Exploring complex data sets using highly engaging environments
Exploring natural user interfaces during meal times
FaceSpace
Getting well and being present
Growing old and staying connected
HandLog: tangible interactions for game input and rehabilitation
iFISH
Improving Vitamin D status and related health in young women
Insertable technology for human interactions
Interactive displays
Interactive spaces and media architecture
Kinecting with orang-utans
Mediating intimacy
Mobile fieldwork and learning
Multimodal human–agent collaboration
Music streaming and algorithmic recommendation
Near-infrared spectroscopy
Onebody
Orygen Virtual World Project
Promoting student peer review in Australian tertiary education
Personal sensing for mental health and wellbeing
Pholiota Unlocked
Reading on ubiquitous devices
Smart Garden Watering
Smartphones for science
Social gaming events: Warhammer 40K
Social networking sites for ambivalent socialisers
Social play in immersive gaming environments
Social robots and virtual assistants for older people
Sociophysical interactions
Social Orientated Requirements Engineering
Spectating eSports and Let’s Play
SpinalLog
Supporting social interactions for video calls in the home
Teleconsultation: enhancing interactions between clinicians and patients
Virtual co-presence
Virtual Reality and climate change communication
VR therapy for youth mental health
Robot Assisted Learning and Rehabilitation
XR for Human-Robot Interaction
Social and Domestic Drones 
Human-Centred Agent Learning
Child of Now
 #thismymob
Publications
Research
Smart Spaces
Ubiquitous computing
Digital health
Digital nature
Novel interactions
Design for ageing
Games and play
Social computing and communities
Human information interaction
Human-Robot Interaction
Seminars
Past seminars 2020
Past seminars 2019
Past seminars 2016–2018
Past seminars 2011–2015
Past seminars 2006–2010
Past seminars 2004–2005
Study
HCI programs for potential students
HCI subjects
Masters projects
Potential PhD students
Information systems
Process science and technology
Business analytics and decision making
Cybersecurity management
Digital health
Innovations in the digital society
Publications
Contact us
Current Students
Library
Staff













====================================================================================================

